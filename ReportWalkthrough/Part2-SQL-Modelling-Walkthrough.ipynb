{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2b797b-a63a-43e5-99dc-be0e8450d88d",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d02384ce-5d98-49d7-aa46-a492945a8d54",
   "metadata": {},
   "source": [
    "In this part, we take our initial transformed data;\n",
    "\n",
    "- HPIInitialTransform.csv\n",
    "- HPINational.csv\n",
    "- MedianPayInitialTransform.csv\n",
    "- MedianSalaryNational.csv\n",
    "- CPIHInitialTransform.csv\n",
    "\n",
    "and load it into MySQL Workbench, to clean and model it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e95c6d41-1448-430b-8816-189cf4ab0fdf",
   "metadata": {},
   "source": [
    "Recall that in the initial cleaning phase, we did not check for missing values, or inconsistent/inappropriate data types (e.g., text used for dates), or verify that related tables represent data in a consistent format (which will be required for facilitating joins).\n",
    "\n",
    "The aim of this next phase is to create a schema resembling the following: (note: dates within the housing tables will be foreign keys, as will all mentions of regions. On the other hand, the date-related fields in CPI and median salary tables will form a many-to-many relationship with the dates dimension table — creating separate dimension tables for years and months felt like overkill).\n",
    "\n",
    "![Schema showing the tables, columns and relationships between them](documentation_images\\ModellingDiagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f68ff9-9259-4927-99ec-24e2e33f7329",
   "metadata": {},
   "source": [
    "A remark on why both regional and national versions of the Housing Prices and Median Salary data exist:\n",
    "\n",
    "- Housing Prices: The sales volumes aren't available for the different types of houses sold (detached, semi-detached, terraced, and flats), so we cannot reconstruct the national average sale price by type without making assumptions about the distribution of each type sold, (and so we must include national level averages for each type).\n",
    "\n",
    "    The `average_price` and `sales_volume` are also included since the national figures for these are available, and we may as well use them (it'll save us doing a calculation later).\n",
    "  \n",
    "- Median Salaries: The median of regional median salaries does not equal the national median salary (this is not due to a data issue, but because of the nature of how medians are calculated). As a result, we require both regional and national median salary statistics for accurate analysis at each level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b32ba2-2b40-4933-97a6-068d5bd7518e",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0b295-5e3e-465f-8a1b-a13fdb729c0d",
   "metadata": {},
   "source": [
    "We begin by loading the data from the previous section (after its initial clean in Excel/PowerQuery). We load the data into MySQL Workbench using the Table Data Import Wizard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01eb3e7-8ba0-426c-b381-435204728348",
   "metadata": {},
   "source": [
    "![A snapshot of the data loaded into MySQL Workbench](documentation_images\\ModellingSQLLoadIn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01948c01-b130-4c33-b415-456e35fb4ece",
   "metadata": {},
   "source": [
    "# Creating the dimension tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a18017-b539-44df-9624-05be109cb222",
   "metadata": {},
   "source": [
    "We start by creating the dimension tables. The Regions table will consist of a single column containing the 12 NUTS regions of the UK: North East, North West, Yorkshire and the Humber, East Midlands, West Midlands, East of England, Greater London, South East, South West, Wales, Scotland, and Northern Ireland.\n",
    "\n",
    "Since we want this table to serve as the source for the correct formatting and spelling of the region names, we will manually create it rather than derive it through the filtering of other tables.\n",
    "\n",
    "This is done as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234082dd-6658-447c-8924-2c5922beaafb",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TABLE RegionsDim (\n",
    "    region varchar(30) primary key\n",
    ");\n",
    "\n",
    "INSERT INTO RegionsDim (region)\n",
    "VALUES\n",
    "(\"North East\"),\n",
    "(\"North West\"), \n",
    "(\"Yorkshire and the Humber\"), \n",
    "(\"East Midlands\"), \n",
    "(\"West Midlands\"),\n",
    "(\"East of England\"),\n",
    "(\"London\"), \n",
    "(\"South East\"), \n",
    "(\"South West\"), \n",
    "(\"Wales\"), \n",
    "(\"Scotland\"), \n",
    "(\"Northern Ireland\");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa8475-c4c4-4e5b-890a-099ac7ace034",
   "metadata": {},
   "source": [
    "Viewing this table to verify:\n",
    "\n",
    "```sql\n",
    "select * from RegionsDim;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbce3a-6a7e-4c36-bc5e-e60389a1f680",
   "metadata": {},
   "source": [
    "![The output of the query `select * from RegionsDim;`](documentation_images\\RegionsDim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1aaeaf-ec01-4a9a-b8e0-2f906c5c0b1d",
   "metadata": {},
   "source": [
    "To build an efficient dates dimension table, we want to find a reasonable minimum and maximum date based on the dates in the fact tables.\n",
    "\n",
    "To do this, we query for the minimum date-related value from each of our five tables (there will be duplicates, but better to be safe than sorry), and visually compare the date-related values. We do the same for the maximum date-related values. Finally, we create the `DatesDim` table based on the identified date range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a50625-c933-4d46-a0c1-959bd5d77e57",
   "metadata": {},
   "source": [
    "\n",
    "```sql\n",
    "--to find the minimum\n",
    "select *\n",
    "from (SELECT MonthYear as d1 FROM cpihinitialtransform LIMIT 1) as t1\n",
    "cross join (SELECT hpiinitialtransform.Date as d2 FROM hpiinitialtransform LIMIT 1) as t2\n",
    "cross join (SELECT hpinational.Date as d3 FROM hpinational LIMIT 1) as t3\n",
    "cross join (SELECT medianpayinitialtransform.Year as d4 FROM medianpayinitialtransform LIMIT 1) as t4\n",
    "cross join (SELECT mediansalarynational.Year as d5 FROM mediansalarynational LIMIT 1) as t5;\n",
    "```\n",
    "Output:\n",
    "![output of querying for the minimum date-related value from each table](documentation_images\\minimum_dates.png)\n",
    "\n",
    "\n",
    "```sql\n",
    "--to find the maximum\n",
    "select *\n",
    "from (SELECT MonthYear as d1 FROM cpihinitialtransform ORDER BY MonthYear DESC LIMIT 1) as t1\n",
    "cross join (SELECT hpiinitialtransform.Date as d2 FROM hpiinitialtransform ORDER BY hpiinitialtransform.Date DESC LIMIT 1) as t2\n",
    "cross join (SELECT hpinational.Date as d3 FROM hpinational ORDER BY hpinational.Date DESC LIMIT 1) as t3\n",
    "cross join (SELECT medianpayinitialtransform.Year as d4 FROM medianpayinitialtransform ORDER BY medianpayinitialtransform.Year DESC LIMIT 1) as t4\n",
    "cross join (SELECT mediansalarynational.Year as d5 FROM mediansalarynational ORDER BY mediansalarynational.Year DESC LIMIT 1) as t5;\n",
    "```\n",
    "Output:\n",
    "![output of querying for the maximum date-related value from each table](documentation_images\\maximum_dates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559ed3f-916a-4f7b-9900-4cd3433c58f1",
   "metadata": {},
   "source": [
    "So, CPI starts in Jan 1988 and ends in Jan 2025, House Prices start in April 1968 and end in Dec 2024, and median pay figures (annual) start in 1999 and end in 2023.\n",
    "\n",
    "Therefore, our minimum date will be `01/04/1968` and our maximum date will be `1/12/2024` (we will discard the first house prices reading of Jan 2025).\n",
    "\n",
    "We now create the date dimension table, `DatesDim`. To do this, we use a `INSERT INTO SELECT` query alongside a recursive CTE to generate a range of dates from `01/04/1988` to `1/12/2024` with monthly intervals. Then use the `MONTH` and `YEAR` functions to extract the month (as a value from 1-12) and the year from each date. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2d1b8-c842-4e59-a1c4-e207e83e4496",
   "metadata": {},
   "source": [
    "```sql\n",
    "create table DatesDim (\n",
    "    date DATE primary key,\n",
    "    month INT,\n",
    "    year INT\n",
    ");\n",
    "\n",
    "insert into DatesDim (date, month, year)\n",
    "\t-- CTE with range of dates from April 1988 to Dec 2024 in yyyy-mm-dd format\n",
    "\twith recursive dates as (\n",
    "\t\tselect CAST('1968-04-01' as date) as d\n",
    "\t\tunion\n",
    "\t\tselect DATE_ADD(d, interval 1 month)\n",
    "\t\tfrom dates\n",
    "\t\twhere d < '2024-12-01'\n",
    "\t)\n",
    "select d, MONTH(d) as month, YEAR(d) as year\n",
    "from dates;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a937c2-3092-4778-9017-1818f82e9f0f",
   "metadata": {},
   "source": [
    "Printing the head of `DatesDim` to verify the rows are as expected:\n",
    "\n",
    "```sql \n",
    "select * from DatesDim limit 5;\n",
    "```\n",
    "\n",
    "Output:\n",
    "![The first five rows of the DatesDim table](documentation_images\\DatesDimHead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d47c9-b5c9-4257-8155-15b7929ef30a",
   "metadata": {},
   "source": [
    "# Facts tables - cleaning, and relationships to the dimension tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c421f497-c00f-4b51-a077-6104285e8ab5",
   "metadata": {},
   "source": [
    "For the final stage of the modelling and cleaning process, we clean the five fact tables and establish relationships with the dimension tables. Since the cleaning steps for the regional and national tables will be largely similar, we will walk through the process only for the regional tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd146a4-b4e6-4e53-8755-e1b0afbf3adc",
   "metadata": {},
   "source": [
    "## House Price tables (regional walkthrough, national is essentially identical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f9ed6-8525-4939-8a80-e81797006d7e",
   "metadata": {},
   "source": [
    "We first query for a description of the regional house price data (`hpiinitialtransform`) to see what datatypes each of the columns have. We also query for a random sample of 10 rows of the data, to see what we are working with (done using `ORDER BY RAND()`):\n",
    "\n",
    "```sql\n",
    "desc hpiinitialtransform;\n",
    "```\n",
    "\n",
    "```sql\n",
    "select * from hpiinitialtransform order by rand();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5be316-929b-4830-84a0-43cf8d80ea68",
   "metadata": {},
   "source": [
    "Outputs:\n",
    "\n",
    "![description of columns of housing price data](documentation_images\\DescriptionHPI.png)\n",
    "\n",
    "![sample of 10 random rows of housing price data](documentation_images\\Sample10HPI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab54cc-d97f-4859-bbd4-d9ecca5b0caa",
   "metadata": {},
   "source": [
    "Since some column datatypes need to be changed, we handle the cleaning process by creating a new table, HousePricesRegional, which includes the converted columns. We also define foreign key constraints linking to the DatesDim and RegionsDim dimension tables to ensure integrity of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c84db-44f8-4641-a9a3-9276cfe5296a",
   "metadata": {},
   "source": [
    "To populate the new table, we use `CREATE TABLE ... AS SELECT`, casting and transforming columns from the `hpiinitialtransform` table. For numeric columns, blank values are replaced with `NULL` rather than `0`, to avoid distorting aggregate functions later on.\n",
    "\n",
    "We also filter the dataset to include only entries up to (and including) the year 2024, since 2025 is incomplete at the time of analysis.\n",
    "\n",
    "```sql\n",
    "create table HousePricesRegional as \n",
    "select\n",
    "    -- convert text to datetime using STR_TO_DATE, then extract the date only using DATE\n",
    "\tDATE(STR_TO_DATE(Date, '%d/%m/%Y %H:%i')) as date,    \n",
    "\tCAST(regionname AS CHAR(50)) as region,\n",
    "    AveragePrice as average_price,\n",
    "    -- for the remaining columns, we convert the text to a double (by using the value in a numerical context). \n",
    "    -- If there is no value, we assign NULL to it.\n",
    "    case\n",
    "\t\twhen salesvolume != '' then salesvolume + 0\n",
    "\t\telse null\n",
    "\tend as sales_volume,\n",
    "\tcase\n",
    "\t\twhen DetachedPrice != '' then DetachedPrice + 0\n",
    "\t\telse null\n",
    "\tend as detached_avg,  \n",
    "\tcase\n",
    "\t\twhen SemiDetachedPrice != '' then SemiDetachedPrice + 0\n",
    "\t\telse null\n",
    "\tend as semi_avg,  \n",
    "\tcase\n",
    "\t\twhen TerracedPrice != '' then TerracedPrice + 0\n",
    "\t\telse null\n",
    "\tend as terraced_avg, \n",
    "\tcase\n",
    "\t\twhen FlatPrice != '' then FlatPrice + 0\n",
    "\t\telse null\n",
    "\tend as flat_avg\n",
    "from hpiinitialtransform\n",
    "where YEAR(DATE(STR_TO_DATE(Date, '%d/%m/%Y %H:%i'))) <= 2024; -- remove 2025 data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda59e9c-f21c-4fdd-9101-2f2504a62536",
   "metadata": {},
   "source": [
    "To ensure intregity of the data, we add foreign key constraints to the `date` and `region` columns, referencing their respective dimension tables. If this works without errors, we know that the `date` and `region` values in the new table are of the required form. \n",
    "\n",
    "![foreign key constraints worked with no errors](documentation_images\\FK-Housing-Works.png)\n",
    "\n",
    "We are now done with the original table, `hpiinitialtranform`, so we drop it to keep our schema tidy.\n",
    "\n",
    "```sql\n",
    "drop table hpiinitialtransform\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1298d1-daa8-4616-a5c1-7d4dd0082041",
   "metadata": {},
   "source": [
    "The query(ies) for creating the national house prices table is almost identical (in fact, the structure is the exact same once all statements involving regions are removed). \n",
    "\n",
    "For completeness, we supply the queries, but do not comment on them further. The new table is called `HousePricesNational`.\n",
    "\n",
    "```sql\n",
    "create table HousePricesNational as \n",
    "select\n",
    "    -- convert text to datetime using STR_TO_DATE, then extract the date only using DATE\n",
    "\tDATE(STR_TO_DATE(Date, '%d/%m/%Y %H:%i')) as date, \n",
    "    AveragePrice as average_price,\n",
    "    -- for the remaining columns, we convert the text to a double (by using the value in a numerical context). \n",
    "    -- If there is no value, we assign NULL to it.\n",
    "    case\n",
    "\t\twhen salesvolume != '' then salesvolume + 0\n",
    "\t\telse null\n",
    "\tend as sales_volume,\n",
    "\tcase\n",
    "\t\twhen DetachedPrice != '' then DetachedPrice + 0\n",
    "\t\telse null\n",
    "\tend as detached_avg,  \n",
    "\tcase\n",
    "\t\twhen SemiDetachedPrice != '' then SemiDetachedPrice + 0\n",
    "\t\telse null\n",
    "\tend as semi_avg,  \n",
    "\tcase\n",
    "\t\twhen TerracedPrice != '' then TerracedPrice + 0\n",
    "\t\telse null\n",
    "\tend as terraced_avg, \n",
    "\tcase\n",
    "\t\twhen FlatPrice != '' then FlatPrice + 0\n",
    "\t\telse null\n",
    "\tend as flat_avg\n",
    "from hpinational\n",
    "where YEAR(DATE(STR_TO_DATE(Date, '%d/%m/%Y %H:%i'))) <= 2024; -- remove 2025 data\n",
    "\n",
    "alter table HousePricesNational\n",
    "ADD FOREIGN KEY (date) REFERENCES DatesDim(date);\n",
    "\n",
    "drop table hpinational;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616dcf2c-ee92-42a1-ad3b-beac90f171bb",
   "metadata": {},
   "source": [
    "## Median Salary tables (regional walkthrough, national is essentially identical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2adfce-ad37-4d4f-8e35-f6adbddf3750",
   "metadata": {},
   "source": [
    "We query for a description as well as a random sample of 10 rows to see how the data from the `medianpayinitialtransform` looks.\n",
    "\n",
    "```sql\n",
    "desc medianpayinitialtransform;\n",
    "\n",
    "select * from medianpayinitialtransform order by rand();\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "\n",
    "![the output of the description and random sample of the median pay regional table](documentation_images\\MedianSalaryFirstLook.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4449f67-9f6b-4657-b01a-9ff18a8655b9",
   "metadata": {},
   "source": [
    "We see that some region names have whitespace before them, this is verified by the following query, so we will have to correct this when cleaning the data (using the `TRIM` function):\n",
    "\n",
    "```sql\n",
    "select region from medianpayinitialtransform where region = '        East';\n",
    "```\n",
    "\n",
    "![output of the whtiespace issue](documentation_images\\WhiteSpaceIssue.png)\n",
    "\n",
    "Otherwise, things look good, so we proceed to create the new table, which we will call `MedianSalaryRegional`. This includes creating a reference to the `RegionsDim` table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e519a91-3683-4782-94bb-0d1d84584ed9",
   "metadata": {},
   "source": [
    "```sql\n",
    "create table MedianSalaryRegional as \n",
    "select\n",
    "    -- convert text to datetime using STR_TO_DATE, then extract the date only using DATE\n",
    "\tcast(TRIM(region) as char(30)) as region, \n",
    "    year as year,\n",
    "    mediansalary as salary\n",
    "from medianpayinitialtransform\n",
    "where year <= 2024; -- remove any 2025 data, if it exists (it doesn't, but just to be safe)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6734420-a8c5-4dea-b62a-e2f4bcacf258",
   "metadata": {},
   "source": [
    "However, when trying to add a foreign key constraint via\n",
    "\n",
    "```sql \n",
    "alter table MedianSalaryRegional\n",
    "add foreign key (region) references RegionsDim(region);\n",
    "```\n",
    "\n",
    "we receive a 1452 error, stating that \"a foreign key constraint fails...\". We can investigate this by querying for the regions which are in the `MedianSalaryRegional` table, but not in the `RegionsDim` table, using the query\n",
    "\n",
    "```sql\n",
    "select distinct region from MedianSalaryRegional where region not in (select region from regionsdim);\n",
    "```\n",
    "\n",
    "This query returns \"East\". Some investigation finds that the issue is that in the RegionsDim table, \"East\" is referred to as \"East of England\". \n",
    "\n",
    "We therefore replace \"East\" with \"East of England\" in the `regions` column of `MedianSalaryRegional` and reattempt the foreign key constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536acd7-169a-4b52-ad44-16248b0a31f2",
   "metadata": {},
   "source": [
    "```sql \n",
    "update MedianSalaryRegional\n",
    "set region = \"East of England\"\n",
    "where region = \"East\";\n",
    "\n",
    "alter table MedianSalaryRegional\n",
    "add foreign key (region) references RegionsDim(region);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afca80a-fa10-423b-bb4d-03c1399a3eb0",
   "metadata": {},
   "source": [
    "It works! We finish with the regional median salary data by dropping the original table:\n",
    "\n",
    "```sql\n",
    "drop table medianpayinitialtransform;\n",
    "```\n",
    "\n",
    "\n",
    "For the national table, there is not much to do. The table `mediansalarynational` is already in the form we want. For example, the columns are of the correct type, as can be seen from querying for a description, and also the maximum year is 2023, which is within the range we want. Therefore, we leave it as it is.\n",
    "\n",
    "```sql\n",
    "desc MedianSalaryNational;\n",
    "\n",
    "-- check that the maximum year isn't 2025 (as we don't want partial 2025 data)\n",
    "select max(year) from MedianSalaryNational;\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "\n",
    "![output of querying description and maximum year of the MedianSalaryNational table](documentation_images\\MedianSalaryNationalOK.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a67898-28eb-40cf-906c-ef7c567cb383",
   "metadata": {},
   "source": [
    "## CPIH table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be505f45-2a01-4be8-8184-a3679727e829",
   "metadata": {},
   "source": [
    "We query for a description and a random sample of 10 rows in order to see what the data looks like.\n",
    "\n",
    "```sql\n",
    "desc cpihinitialtransform;\n",
    "\n",
    "select * from cpihinitialtransform order by rand();\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "\n",
    "![the output of querying for a description and random sample from the cpi table](documentation_images\\CPIFirstLook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5f503-8c40-4816-90ea-bb2840af21b1",
   "metadata": {},
   "source": [
    "We can see that `MonthYear` is of type \"Text\", and the format is an abbreviated month name and then a 4-digit year separated by a space. This is given as \"%b %Y\" as a format string. We use this alongside the `STR_TO_DATE` function to convert the column to datetimes, then we use `MONTH` and `YEAR` to create month and year columns. \n",
    "\n",
    "We make use of a CTE within the `create table` statement so that we only have to convert the text to dates once.\n",
    "\n",
    "We also add a column normalising the CPI indices so that Dec 2024's index is 1 (and so most normalised historical indices will be <1 - this will be useful when adjusting housing prices by CPIH later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c9ae0-303b-4590-9285-9606799de11e",
   "metadata": {},
   "source": [
    "```sql\n",
    "create table CPIH as \n",
    "\twith formatted_date as\n",
    "\t\t(\n",
    "\t\tselect DATE(str_to_date(concat(MonthYear,' 01'), \"%Y %b %d\")) as f_date, `CPI-Index` from cpihinitialtransform\n",
    "\t\t)\n",
    "\tselect\n",
    "\t\tMONTH(f_date) as month, \n",
    "\t\tYEAR(f_date) as year, \n",
    "\t\t`CPI-Index` as indx, -- to avoid error with using the index keyword (i.e. indx instead of index).\n",
    "        -- normalise by the most recent CPI-Index, unless it is more recent than 2024-12-01. If so, normalise by CPI-Index from 2024-12-01 instead.\n",
    "\t\t`CPI-Index`/(select `CPI-Index` \n",
    "                    from formatted_date where f_date = (select least(max(f_date),\"2024-12-01\") from formatted_date)\n",
    "                    ) as index_normed\n",
    "\tfrom formatted_date\n",
    "\twhere YEAR(f_date) <= 2024; -- remove 2025 data if it exists\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e06d35-6ff2-4d5f-a81f-525ca0269d0c",
   "metadata": {},
   "source": [
    "A sample from the newly created CPIH table shows that the creation has worked as intended. We can drop the old CPI table, namely `cpiinitialtransform`, from our schema to keep it tidy.\n",
    "\n",
    "```sql\n",
    "select * from CPIH limit 10;\n",
    "    \n",
    "drop table cpihinitialtransform;\n",
    "```\n",
    "\n",
    "![a sample of the new CPI with the dates in a more workable format](documentation_images\\NewCPISample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a20aee-4fdf-4411-bddd-434b6a592f7e",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebad13-2c92-4cb3-b6fb-bca1bd7afef1",
   "metadata": {},
   "source": [
    "The modelling phase is now finished. Our final schema is as follows:\n",
    "\n",
    "![image of the final schema](documentation_images\\FinalSchema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1485bd-8bd0-4462-b206-77ba0efdb524",
   "metadata": {},
   "source": [
    "Next, we’ll focus on generating insights using SQL and then move on to creating a report in Power BI. In Power BI, we’ll likely need to create a separate dimension table for `Year` to aid in filtering and to ensure the correct context for visuals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
